# Content Recommendation System - Testing Setup Guide

## é²œæ˜æ¦‚è¿° | Quick Overview

This guide explains how to set up and test the Content Recommendation System with automatic keyword-based content crawling and scheduled delivery.

## å‰ç½®è¦æ±‚ | Prerequisites

- Python 3.8+
- pip (Python package manager)
- Git
- Text editor or IDE

## 1. ç¯å¢ƒé…ç½® | Environment Configuration

### 1.1 å¤åˆ¶ç¯å¢ƒå˜é‡ | Copy Environment Variables

```bash
cp .env.example .env
```

### 1.2 ä¿®æ”¹ .env æ–‡ä»¶ | Edit .env File

The `.env` file has been configured for local testing with the following key settings:

```
# FastAPI Configuration
FAST_API_HOST=0.0.0.0
FAST_API_PORT=8000
DEBUG=True
ENVIRONMENT=development

# Database (using SQLite for testing)
DATABASE_URL=sqlite:///./test.db
```

**Note:** For production use with Supabase, update the database configuration accordingly.

## 2. å®‰è£…ä¾èµ– | Install Dependencies

### 2.1 åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ | Create Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 2.2 å®‰è£…æ‰€éœ€åŒ… | Install Required Packages

```bash
pip install -r requirements.txt
```

## 3. å¯åŠ¨åç«¯æœåŠ¡ | Start Backend Service

### 3.1 è¿è¡Œ FastAPI æœåŠ¡å™¨ | Run FastAPI Server

```bash
python app.py
```

Expected output:
```
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

### 3.2 éªŒè¯æœåŠ¡å¥åº·çŠ¶å†µ | Verify Service Health

Open your browser and visit:
- Health check: `http://localhost:8000/health`
- API documentation: `http://localhost:8000/docs`

## 4. è‡ªåŠ¨çˆ¬è™«é…ç½® | Automatic Crawler Configuration

### 4.1 ä½¿ç”¨æµ‹è¯•é…ç½® | Using Test Configuration

The system provides a test crawler configuration file: `test_crawler_config.json`

This configuration includes:

#### **å°çº¢ä¹¦ (Xiaohongshu) çˆ¬è™«é…ç½®**

Keywords: Pythonç¼–ç¨‹, Webå¼€å‘, æ•°æ®åˆ†æ, æœºå™¨å­¦ä¹ , å‰ç«¯æ¡†æ¶

Schedule: Every 6 hours automatically

#### **æŠ–éŸ³ (Douyin) çˆ¬è™«é…ç½®**

Keywords: ç¼–ç¨‹æ•™ç¨‹, å¼€å‘å·¥å…·, æŠ€æœ¯åˆ†äº«, ä»£ç ç‰‡æ®µ, é¡¹ç›®åˆ†äº«

Schedule: Every 12 hours automatically

### 4.2 å¯åŠ¨çˆ¬è™« | Start Crawler

#### Option A: ä½¿ç”¨ API ç«¯ç‚¹ | Using API Endpoint

```bash
curl -X POST "http://localhost:8000/api/crawler/start" \
  -H "Content-Type: application/json" \
  -d '{
    "platform": "xiaohongshu",
    "keywords": ["Pythonç¼–ç¨‹", "Webå¼€å‘"],
    "count": 50,
    "max_pages": 2
  }'
```

#### Option B: ä½¿ç”¨ Swagger UI | Using Swagger UI

1. Open `http://localhost:8000/docs`
2. Find the "POST /api/crawler/start" endpoint
3. Click "Try it out"
4. Enter the configuration and click "Execute"

## 5. æµ‹è¯•æ•°æ®æµ | Test Data Flow

### 5.1 æŸ¥çœ‹çˆ¬å–çš„å†…å®¹ | View Crawled Content

```bash
curl "http://localhost:8000/api/content/list?platform=xiaohongshu&limit=10"
```

### 5.2 è·å–ç³»ç»Ÿç»Ÿè®¡ | Get System Statistics

```bash
curl "http://localhost:8000/api/stats"
```

### 5.3 è‡ªåŠ¨æ ‡ç­¾ç”Ÿæˆ | Auto-Tagging

```bash
curl -X POST "http://localhost:8000/api/tagging/auto-tag" \
  -H "Content-Type: application/json" \
  -d '{
    "content_id": "test_001",
    "title": "Python Web Development Tutorial",
    "description": "Learn Django and FastAPI",
    "hashtags": ["python", "web", "tutorial"]
  }'
```

## 6. è®¿é—®æµ‹è¯•ç•Œé¢ | Access Testing UI

### 6.1 æ‰“å¼€æµ‹è¯•ä»ªè¡¨æ¿ | Open Testing Dashboard

Access the web interface at:
```
http://localhost:8080/content_analysis.html
```

### 6.2 ä¸»è¦åŠŸèƒ½ | Main Features

- **å†…å®¹è¾“å…¥é¢æ¿ | Content Input Panel**: Enter keywords for manual crawling
- **çˆ¬è™«é…ç½® | Crawler Configuration**: Set up scheduling and sources
- **å†…å®¹é¢„è§ˆ | Content Preview**: View fetched content
- **æ ‡ç­¾ç®¡ç† | Tag Management**: Manage content tags
- **ç»Ÿè®¡ä»ªè¡¨æ¿ | Statistics Dashboard**: View system metrics

## 7. è®¡åˆ’ä»»åŠ¡é…ç½® | Scheduled Task Configuration

### 7.1 å¯ç”¨å®šæ—¶çˆ¬è™« | Enable Scheduled Crawling

The test configuration includes automatic scheduling:

```json
{
  "schedule": {
    "enabled": true,
    "interval_hours": 6,
    "description": "æ¯6å°æ—¶è‡ªåŠ¨çˆ¬å–ä¸€æ¬¡"
  }
}
```

### 7.2 è‡ªå®šä¹‰è®¡åˆ’ | Customize Schedule

Edit `test_crawler_config.json` to change:
- `interval_hours`: Schedule interval in hours
- `keywords`: Content keywords to crawl
- `platform`: Target platform (xiaohongshu, douyin, etc.)

## 8. æ•…éšœæ’æŸ¥ | Troubleshooting

### é—®é¢˜1ï¼šè¿æ¥è¢«æ‹’ç» | Connection Refused

**ç—‡çŠ¶ï¼š** `ERR_CONNECTION_REFUSED`

**è§£å†³æ–¹æ¡ˆï¼š**
1. Ensure backend is running: `python app.py`
2. Check if port 8000 is available
3. Verify `.env` configuration

### é—®é¢˜2ï¼šæ•°æ®åº“é”™è¯¯ | Database Errors

**ç—‡çŠ¶ï¼š** Database connection errors

**è§£å†³æ–¹æ¡ˆï¼š**
1. Delete old `test.db` file: `rm test.db`
2. Restart the backend
3. Database will be created automatically

### é—®é¢˜3ï¼šçˆ¬è™«æœªå¯åŠ¨ | Crawler Not Starting

**ç—‡çŠ¶ï¼š** Crawler returns `status: started` but no content appears

**è§£å†³æ–¹æ¡ˆï¼š**
1. Check crawler logs in the backend console
2. Verify API credentials if using real platforms
3. Try with test keywords first

## 9. API ç«¯ç‚¹å‚è€ƒ | API Endpoints Reference

| æ–¹æ³• | ç«¯ç‚¹ | è¯´æ˜ |
|------|------|------|
| GET | `/health` | å¥åº·æ£€æŸ¥ |
| POST | `/api/content/upload` | ä¸Šä¼ å†…å®¹ |
| GET | `/api/content/list` | è·å–å†…å®¹åˆ—è¡¨ |
| POST | `/api/tagging/auto-tag` | è‡ªåŠ¨æ ‡ç­¾ |
| POST | `/api/persona/create` | åˆ›å»ºäººç¾¤ç”»åƒ |
| POST | `/api/crawler/start` | å¯åŠ¨çˆ¬è™« |
| POST | `/api/feishu/sync` | åŒæ­¥åˆ°é£ä¹¦ |
| GET | `/api/stats` | è·å–ç»Ÿè®¡ä¿¡æ¯ |

## 10. ä¸‹ä¸€æ­¥ | Next Steps

1. âœ… Environment configuration complete
2. âœ… Backend service running
3. âœ… Test crawler configuration ready
4. ğŸ”„ Run initial crawl with keywords
5. ğŸ”„ Monitor scheduled tasks
6. ğŸ”„ Integrate with Feishu for notifications

## æ”¯æŒ | Support

For issues or questions:
1. Check `TESTING_GUIDE.md` for detailed testing procedures
2. Review `API.md` for API documentation
3. Check backend logs for errors
4. Refer to `QUICK_START.md` for quick setup
