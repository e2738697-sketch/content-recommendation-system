#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦å†…å®¹çˆ¬è™«é€‚é…å™¨
åŸºäº MediaCrawler æ¶æ„ï¼Œé€‚é… Supabase å­˜å‚¨
ä¼˜åŒ–ç‰ˆï¼šæ•°æ®ç»“æ„åŒ¹é…Supabaseè¡¨ç»“æ„
"""

import os
import json
import asyncio
from datetime import datetime
from typing import List, Dict, Any
import sys

# Supabase å®¢æˆ·ç«¯
try:
    from supabase import create_client, Client
except ImportError:
    print("Error: supabase package not installed. Run: pip install supabase")
    sys.exit(1)


class XiaohongshuCrawler:
    """
    å°çº¢ä¹¦å†…å®¹çˆ¬è™«
    
    åŠŸèƒ½:
    1. æœç´¢æŒ‡å®šå…³é”®è¯çš„ç¬”è®°
    2. è·å–ç¬”è®°è¯¦æƒ…å’Œè¯„è®º
    3. å­˜å‚¨åˆ° Supabase æ•°æ®åº“ï¼ˆåŒ¹é…content_rawè¡¨ç»“æ„ï¼‰
    """
    
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«"""
        # ä»ç¯å¢ƒå˜é‡è·å– Supabase é…ç½®
        supabase_url = os.getenv('SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_KEY')
        
        if not supabase_url or not supabase_key:
            raise ValueError("Missing SUPABASE_URL or SUPABASE_KEY environment variables")
        
        # åˆå§‹åŒ– Supabase å®¢æˆ·ç«¯
        self.supabase: Client = create_client(supabase_url, supabase_key)
        
        # æœç´¢å…³é”®è¯ï¼ˆå¯ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        self.keywords = self._load_keywords()
        
        print(f"âœ… å°çº¢ä¹¦çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
        print(f"ğŸ“Œ æœç´¢å…³é”®è¯: {self.keywords}")
    
    def _load_keywords(self) -> List[str]:
        """
        ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®åŠ è½½æœç´¢å…³é”®è¯
        """
        # å°è¯•ä»ç¯å¢ƒå˜é‡åŠ è½½
        keywords_str = os.getenv('SEARCH_KEYWORDS', '')
        if keywords_str:
            return [k.strip() for k in keywords_str.split(',')]
        
        # é»˜è®¤å…³é”®è¯
        return ['å±…å®¶å¥½ç‰©', 'æ•°ç æµ‹è¯„', 'ç¾é£Ÿæ¢åº—']
    
    async def crawl(self):
        """
        æ‰§è¡Œçˆ¬å–ä»»åŠ¡
        """
        print("\nğŸš€ å¼€å§‹çˆ¬å–å°çº¢ä¹¦å†…å®¹...")
        
        try:
            # ç”Ÿæˆç¤ºä¾‹æ•°æ®ï¼ˆå®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºçœŸå®çˆ¬å–ï¼‰
            sample_data = self._generate_sample_data()
            
            # å­˜å‚¨åˆ° Supabase
            await self._save_to_database(sample_data)
            
            print(f"\nâœ… çˆ¬å–å®Œæˆï¼å…±é‡‡é›† {len(sample_data)} æ¡å†…å®¹")
            
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥: {str(e)}")
            raise
    
    def _generate_sample_data(self) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆç¤ºä¾‹æ•°æ®ï¼ˆåŒ¹é…Supabase content_rawè¡¨ç»“æ„ï¼‰
        
        è¡¨ç»“æ„:
        - platform: character varying(50)
        - content_id: character varying(255)
        - author_id: character varying(255)
        - author_name: character varying(255)
        - title: text
        - text: text
        - hashtags: jsonb (é»˜è®¤ '[]')
        - media_urls: jsonb (é»˜è®¤ '[]')
        - like_count: integer
        - collect_count: integer
        - comment_count: integer
        - share_count: integer
        - view_count: integer
        - publish_time: timestamp
        - fetch_time: timestamp (é»˜è®¤ now())
        """
        sample_data = []
        timestamp = datetime.now()
        
        for keyword in self.keywords[:2]:  # æ¯ä¸ªå…³é”®è¯çˆ¬å–2æ¡ç¤ºä¾‹
            for i in range(2):
                item = {
                    'platform': 'xiaohongshu',
                    'content_id': f'xhs_{keyword}_{i}_{int(timestamp.timestamp())}',
                    'author_id': f'author_{i+1}',
                    'author_name': f'ç”¨æˆ·{i+1}',
                    'title': f'{keyword} ç›¸å…³ç¬”è®° {i+1}',
                    'text': f'è¿™æ˜¯å…³äº {keyword} çš„ç²¾å½©å†…å®¹åˆ†äº«ï¼éå¸¸å®ç”¨ï¼Œå¼ºçƒˆæ¨èç»™å¤§å®¶ã€‚',
                    'hashtags': json.dumps([keyword, 'æ¨è', 'ç§è‰']),
                    'media_urls': json.dumps([f'https://example.com/image_{i}_1.jpg', f'https://example.com/image_{i}_2.jpg']),
                    'like_count': 1000 + i * 100,
                    'collect_count': 200 + i * 50,
                    'comment_count': 50 + i * 10,
                    'share_count': 30 + i * 5,
                    'view_count': 5000 + i * 500,
                    'publish_time': timestamp.isoformat()
                    # fetch_time ä¼šç”±æ•°æ®åº“è‡ªåŠ¨è®¾ç½®ä¸º now()
                }
                sample_data.append(item)
        
        return sample_data
    
    async def _save_to_database(self, data: List[Dict[str, Any]]):
        """
        ä¿å­˜æ•°æ®åˆ° Supabase content_raw è¡¨
        """
        print(f"\nğŸ’¾ å¼€å§‹å­˜å‚¨æ•°æ®åˆ° Supabase...")
        
        success_count = 0
        fail_count = 0
        
        for idx, item in enumerate(data, 1):
            try:
                # æ’å…¥åˆ° content_raw è¡¨
                result = self.supabase.table('content_raw').insert(item).execute()
                print(f"  [{idx}/{len(data)}] âœ“ å·²å­˜å‚¨: {item['title']}")
                success_count += 1
            
            except Exception as e:
                print(f"  [{idx}/{len(data)}] âœ— å­˜å‚¨å¤±è´¥: {str(e)}")
                fail_count += 1
        
        print(f"\nğŸ’¾ æ•°æ®å­˜å‚¨å®Œæˆ - æˆåŠŸ: {success_count}, å¤±è´¥: {fail_count}")


async def main():
    """
    ä¸»å‡½æ•°
    """
    print("="*60)
    print("   å°çº¢ä¹¦å†…å®¹çˆ¬è™« - Supabase é€‚é…ç‰ˆ (ä¼˜åŒ–)")
    print("="*60)
    
    try:
        crawler = XiaohongshuCrawler()
        await crawler.crawl()
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        sys.exit(1)


if __name__ == '__main__':
    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main())
